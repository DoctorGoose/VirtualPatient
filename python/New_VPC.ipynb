{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "from math import ceil\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import pandas as pd\n",
    "from scipy.integrate import solve_ivp\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "df_Rahil = pd.read_excel(\"../Data/References/Rahil_2020/Table8.xlsx\", usecols=list(range(0, 143)))\n",
    "df_Rahil = df_Rahil.loc[df_Rahil['DAY'] <= 8]\n",
    "all_ids = df_Rahil['VOLUNTEER'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeManager:\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def check_timeout(self, timeout: int) -> bool:\n",
    "        return (time.time() - self.start_time) > timeout\n",
    "\n",
    "    def reset_start_time(self):\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def get_elapsed_time(self) -> float:\n",
    "        return time.time() - self.start_time\n",
    "\n",
    "class Parameter:\n",
    "    def __init__(self, name, val, l_lim=None, u_lim=None, dist='uniform', mode='fixed', space='log10'):\n",
    "        self.name = name\n",
    "        self.val = val\n",
    "        self.l_lim = l_lim\n",
    "        self.u_lim = u_lim\n",
    "        self.dist = dist\n",
    "        self.mode = mode\n",
    "        self.space = space\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Parameter(name={self.name}, val={self.val}, bounds=({self.l_lim}, {self.u_lim}), dist={self.dist}, mode={self.mode}, space={self.space})\"\n",
    "\n",
    "    def sample_value(self):\n",
    "        if self.dist == 'uniform':\n",
    "            return np.random.uniform(self.l_lim, self.u_lim)\n",
    "        elif self.dist == 'loguniform':\n",
    "            return 10**np.random.uniform(np.log10(self.l_lim), np.log10(self.u_lim))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown distribution type: {self.dist}\")\n",
    "\n",
    "class Parameters:\n",
    "    def __init__(self, **kwargs):\n",
    "        self._parameters = kwargs\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        if item in self._parameters:\n",
    "            return self._parameters[item]\n",
    "        raise AttributeError(f\"'Parameters' object has no attribute '{item}'\")  # Base case to prevent recursion\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        if key == '_parameters':\n",
    "            super().__setattr__(key, value)\n",
    "        else:\n",
    "            self._parameters[key] = value\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # Return the state to be pickled\n",
    "        return self._parameters\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # Restore state from the unpickled state\n",
    "        self._parameters = state\n",
    "\n",
    "    def get_sampled_parameters(self):\n",
    "        return {name: param for name, param in self._parameters.items() if param.mode == 'sample'}\n",
    "\n",
    "    def items(self):\n",
    "        return self._parameters.items()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Parameters({', '.join([f'{k}={v}' for k, v in self._parameters.items()])})\"\n",
    "\n",
    "    def load_parameters_from_file(self, path, usecols=None):\n",
    "        df = pd.read_excel(path, usecols=usecols)\n",
    "        \n",
    "        # Filter out rows where 'ID' contains non-numeric values like 'Mean', 'STD', etc.\n",
    "        df = df[~df['id'].astype(str).str.contains('Mean|STD|Min|Max', regex=True)]\n",
    "        \n",
    "        file_parameters = []\n",
    "\n",
    "        # Loop through each row in the DataFrame\n",
    "        for index, row in df.iterrows():\n",
    "            param_dict = {}\n",
    "            \n",
    "            # Loop through each parameter in the initial template\n",
    "            for name, param in self._parameters.items():\n",
    "                if param.mode == 'file':\n",
    "                    if name in df.columns and not pd.isna(row[name]):\n",
    "                        # Use the CSV value if the column exists and the value is not NaN\n",
    "                        param_dict[name] = Parameter(name=name, val=row[name], l_lim=param.l_lim, u_lim=param.u_lim,\n",
    "                                                     dist=param.dist, mode='fixed', space=param.space)\n",
    "                    else:\n",
    "                        # Use the default value from the initial template\n",
    "                        print(f'Parameter {name} in file mode resorted to the default fixed value.')\n",
    "                        param_dict[name] = param\n",
    "                else:\n",
    "                    param_dict[name] = param\n",
    "\n",
    "            # Create a new Parameters object for this row\n",
    "            file_parameters.append(Parameters(**param_dict))\n",
    "\n",
    "        return file_parameters\n",
    "\n",
    "class State:\n",
    "    def __init__(self, label, initial_value=0.0):\n",
    "        self.label = label\n",
    "        self.initial_value = initial_value\n",
    "        self.time_points = np.array([0.0])\n",
    "        self.values = np.array([initial_value])\n",
    "\n",
    "    def update_value(self, t, new_value):\n",
    "        self.time_points = np.append(self.time_points, t)\n",
    "        self.values = np.append(self.values, new_value)\n",
    "\n",
    "    def get_latest_value(self):\n",
    "        return self.values[-1]\n",
    "\n",
    "    def get_value_at(self, t_delay):\n",
    "        return np.interp(t_delay, self.time_points, self.values, left=self.initial_value, right=self.values[-1])\n",
    "\n",
    "    def reset(self):\n",
    "        self.time_points = np.array([0.0])\n",
    "        self.values = np.array([self.initial_value])\n",
    "\n",
    "class States:\n",
    "    def __init__(self, states_config):\n",
    "        self.states = {config['label']: State(**config) for config in states_config}\n",
    "        self.tau = {}\n",
    "        self.state_labels = [state.label for state in self.states.values()]\n",
    "\n",
    "    def get_current_values_as_array(self):\n",
    "        return np.array([self.states[label].get_latest_value() for label in self.state_labels])\n",
    "\n",
    "    def get_delayed_state(self, state_label):\n",
    "        return self.tau.get(state_label, 0)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name in self.states:\n",
    "            return self.states[name].get_latest_value()\n",
    "        elif name in self.tau:\n",
    "            return self.tau[name]\n",
    "        else:\n",
    "            raise AttributeError(f\"'States' object has no attribute '{name}'\")\n",
    "\n",
    "    def update_states(self, t, new_values):\n",
    "        # Vectorized state update\n",
    "        for label, value in zip(self.state_labels, new_values):\n",
    "            self.states[label].update_value(t, value)\n",
    "\n",
    "    def calculate_delayed_states(self, t, delays, p):\n",
    "        # Batch process delayed state calculations\n",
    "        for delay_info in delays:\n",
    "            tau, dependent_state, affecting_state = delay_info\n",
    "            delay_time = getattr(p, tau).val\n",
    "            delayed_value = self.states[affecting_state].get_value_at(t - delay_time)\n",
    "            delayed_state_label = f\"{affecting_state}_{dependent_state}\"\n",
    "            self.tau[delayed_state_label] = delayed_value\n",
    "\n",
    "    def reset_delayed_states(self):\n",
    "        self.tau.clear()\n",
    "\n",
    "def SmartSolve(task):\n",
    "    funx, param_set, states_config, t_span = task\n",
    "    states = States(states_config)\n",
    "    y_initial = states.get_current_values_as_array()\n",
    "    solvers_with_timeouts = [('RK45', 1.5), ('BDF', 5.0)]\n",
    "\n",
    "    time_manager = TimeManager() # Process hang monitoring init\n",
    "    warnings.filterwarnings(\"ignore\", message=\"The following arguments have no effect for a chosen solver:*\") # Suppress warnings\n",
    "    \n",
    "    # Custom event for overflow detection\n",
    "    def overflow_event(t, y):\n",
    "        return 1E12 - max(abs(yi) for yi in y)  \n",
    "    overflow_event.terminal = True\n",
    "    overflow_event.direction = -1\n",
    "\n",
    "    if np.shape(t_span)[0] > 2:\n",
    "        t_eval = t_span[1:-1]  # Assuming this should include all but the first and last for evaluation\n",
    "        t_span = [t_span[0], t_span[-1]]\n",
    "    else:\n",
    "        t_eval = None\n",
    "    \n",
    "    for method, timeout in solvers_with_timeouts:\n",
    "        try:\n",
    "            sol = solve_ivp(\n",
    "                fun=lambda t, y: funx(t, y, param_set, states, time_manager, timeout=timeout),\n",
    "                t_span=t_span, y0=y_initial, method=method, t_eval=t_eval, dense_output=False,\n",
    "                events=overflow_event, vectorized=True, rtol=1e-5, atol=1e-6, max_step=0.1, min_step=1e-5\n",
    "            )\n",
    "            print('test')\n",
    "            if sol.success:\n",
    "                elapsed_time = time_manager.get_elapsed_time()\n",
    "                return sol, elapsed_time, param_set\n",
    "            else:\n",
    "                continue\n",
    "        except TimeoutError:\n",
    "            continue\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return None, time_manager.get_elapsed_time(), param_set\n",
    "\n",
    "def batched_parallel_execution(funx, parameter_sets, states_config, t_span, min_tasks_per_core=1):\n",
    "    total_cores = max(1, multiprocessing.cpu_count() - 2)\n",
    "    total_tasks = len(parameter_sets)\n",
    "    optimal_batches = max(1, min(total_cores, ceil(total_tasks / min_tasks_per_core)))\n",
    "    batch_size = ceil(total_tasks / optimal_batches)\n",
    "\n",
    "    batches = [parameter_sets[i:i + batch_size] for i in range(0, total_tasks, batch_size)]\n",
    "\n",
    "    def process_batch(batch):\n",
    "        results = []\n",
    "        for param_set in batch:\n",
    "            task = (funx, param_set, states_config, t_span)\n",
    "            result = SmartSolve(task)\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "    cpu_count = optimal_batches\n",
    "    aggregated_results = Parallel(n_jobs=cpu_count)(\n",
    "        delayed(process_batch)(batch) for batch in batches\n",
    "    )\n",
    "\n",
    "    return [item for sublist in aggregated_results for item in sublist]\n",
    "\n",
    "def execute(funx, parameter_sets, states_config, t_span, parallel=True):\n",
    "    sol_list = []\n",
    "    integration_times = []\n",
    "    used_param_sets = []\n",
    "\n",
    "    def handle_solution(sol, integration_time, param_set, sol_list, integration_times, used_param_sets):\n",
    "            sol_list.append(sol)\n",
    "            integration_times.append(integration_time)\n",
    "            used_param_sets.append(param_set)\n",
    "\n",
    "    if parallel:\n",
    "        results = batched_parallel_execution(funx, parameter_sets, states_config, t_span)\n",
    "\n",
    "        for result in results:\n",
    "            sol, integration_time, param_set = result\n",
    "            handle_solution(sol, integration_time, param_set, sol_list, integration_times, used_param_sets)\n",
    "    else:\n",
    "        for param_set in parameter_sets:\n",
    "            result = SmartSolve((funx, param_set, states_config, t_span))\n",
    "            sol, integration_time, param_set = result\n",
    "            handle_solution(sol, integration_time, param_set, sol_list, integration_times, used_param_sets)\n",
    "\n",
    "    results_dict = {\n",
    "        \"run_id\": datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"),\n",
    "        \"parameter_sets\": used_param_sets,\n",
    "        \"sol_list\": sol_list,\n",
    "        \"integration_times\": integration_times\n",
    "    }\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "def load_parameters_from_file(self, path, usecols=None):\n",
    "        df = pd.read_excel(path, usecols=usecols)\n",
    "        \n",
    "        # Filter out rows where 'ID' contains non-numeric values like 'Mean', 'STD', etc.\n",
    "        df = df[~df['id'].astype(str).str.contains('Mean|STD|Min|Max', regex=True)]\n",
    "        \n",
    "        file_parameters = []\n",
    "\n",
    "        # Loop through each row in the DataFrame\n",
    "        for index, row in df.iterrows():\n",
    "            param_dict = {}\n",
    "            \n",
    "            # Loop through each parameter in the initial template\n",
    "            for name, param in self._parameters.items():\n",
    "                if param.mode == 'file':\n",
    "                    if name in df.columns and not pd.isna(row[name]):\n",
    "                        # Use the CSV value if the column exists and the value is not NaN\n",
    "                        param_dict[name] = Parameter(name=name, val=row[name], l_lim=param.l_lim, u_lim=param.u_lim,\n",
    "                                                     dist=param.dist, mode='fixed', space=param.space)\n",
    "                    else:\n",
    "                        # Use the default value from the initial template\n",
    "                        print(f'Parameter {name} in file mode resorted to the default fixed value.')\n",
    "                        param_dict[name] = param\n",
    "                else:\n",
    "                    param_dict[name] = param\n",
    "\n",
    "            # Create a new Parameters object for this row\n",
    "            file_parameters.append(Parameters(**param_dict))\n",
    "\n",
    "        return file_parameters\n",
    "\n",
    "def generate_parameter_sets(n_sets, parameters, include_initial=False):\n",
    "    sampled_params = parameters.get_sampled_parameters()\n",
    "    \n",
    "    num_vars = len(sampled_params)\n",
    "    param_names = list(sampled_params.keys())\n",
    "    samples = np.zeros((n_sets, num_vars))\n",
    "    \n",
    "    # Generate samples for each parameter\n",
    "    for i, param_name in enumerate(param_names):\n",
    "        param = sampled_params[param_name]\n",
    "        if param.space == 'log10':\n",
    "            low, high = np.log10(param.l_lim), np.log10(param.u_lim)\n",
    "        else:\n",
    "            low, high = param.l_lim, param.u_lim\n",
    "\n",
    "        if param.dist == 'uniform':\n",
    "            samples[:, i] = np.random.uniform(low, high, n_sets)\n",
    "        elif param.dist == 'normal':\n",
    "            mean, stddev = (low + high) / 2, (high - low) / 6\n",
    "            samples[:, i] = norm.rvs(loc=mean, scale=stddev, size=n_sets)\n",
    "\n",
    "        if param.space == 'log10':\n",
    "            samples[:, i] = 10 ** samples[:, i]\n",
    "            \n",
    "    parameter_sets = []\n",
    "    \n",
    "    for set_row in samples:\n",
    "        # Create a dictionary for the new parameter values\n",
    "        new_param_dict = parameters._parameters.copy()  # Assuming _parameters is accessible; adjust as needed\n",
    "\n",
    "        # Update the dictionary with the new sampled values\n",
    "        for i, param_name in enumerate(param_names):\n",
    "            sampled_value = set_row[i]\n",
    "            new_param_dict[param_name] = Parameter(name=param_name,\n",
    "                                                  val=sampled_value,\n",
    "                                                  l_lim=sampled_params[param_name].l_lim,\n",
    "                                                  u_lim=sampled_params[param_name].u_lim,\n",
    "                                                  dist=sampled_params[param_name].dist,\n",
    "                                                  sample=sampled_params[param_name].sample,\n",
    "                                                  space=sampled_params[param_name].space)\n",
    "\n",
    "        # Create a new Parameters instance with the updated dictionary\n",
    "        new_parameters = Parameters(**new_param_dict)\n",
    "        parameter_sets.append(new_parameters)\n",
    "        \n",
    "    if include_initial:\n",
    "        parameter_sets.append(parameters)\n",
    "    return parameter_sets\n",
    "\n",
    "def display_legend(color_dict):\n",
    "    # Creating a figure just for the legend\n",
    "    fig, ax = plt.subplots(figsize=(5, len(color_dict) * 0.3))  # Adjust size as needed\n",
    "    ax.axis('off')  # Turn off axis\n",
    "    patches = [plt.Line2D([0], [0], color=color, marker='o', linestyle='', markersize=10, label=f'{volunteer}') for volunteer, color in color_dict.items()]\n",
    "    legend = ax.legend(handles=patches, loc='center', frameon=False)\n",
    "    plt.show()\n",
    "    \n",
    "def unique_colors(ids):\n",
    "    num_colors = len(ids)\n",
    "    colors = sns.color_palette(\"husl\", num_colors) \n",
    "    return dict(zip(ids, colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_ids = [103, 107, 108, 111, 112, 207, 301, 302, 307, 308, 311, 312]\n",
    "best_ids = [111, 112, 207, 302, 308, 312]\n",
    "best_Rahil = df_Rahil[df_Rahil['VOLUNTEER'].isin(best_ids)]\n",
    "best_color_dict = unique_colors(best_ids)\n",
    "t_span = [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]\n",
    "t_fill = np.linspace(t_span[0], t_span[-1], 50)\n",
    "t_span = np.unique(np.concatenate([t_span, t_fill])) \n",
    "\n",
    "def MonoModel(t: float, y: np.ndarray, p: Parameters, s: States, time_manager: TimeManager, timeout: int = None) -> np.ndarray:\n",
    "    if timeout is not None and time_manager.check_timeout(timeout): \n",
    "        raise TimeoutError('ODE Solver timeout.')\n",
    "\n",
    "    y[:] = y.clip(min=0.0)\n",
    "    for label, value in zip(s.state_labels, y):\n",
    "        s.states[label].update_value(t, value)\n",
    "\n",
    "    s.calculate_delayed_states(t, [('tau_E', 'E', 'I2'), \n",
    "                                ('tau_EM', 'EM', 'E')\n",
    "                                ], p)\n",
    "    I2_tauE = s.get_delayed_state('I2_E')\n",
    "    E_tauEM = s.get_delayed_state('E_EM') \n",
    "\n",
    "    return np.array([\n",
    "        -p.beta.val * s.T * s.V,  # T\n",
    "        p.beta.val * s.T * s.V - p.k.val * s.I1,  # I1\n",
    "        p.k.val * s.I1 - p.delta.val * s.I2, #- p.delta_E.val * s.E * s.I2 / (p.Kd.val + s.I2),  # I2\n",
    "        p.p.val * s.I2 - p.c.val * s.V,  # V\n",
    "        p.xi.val * s.I2 / (p.K_Ef.val + s.E) + p.eta.val * s.E * I2_tauE - p.d_E.val * s.E,  # E\n",
    "        p.zet.val * E_tauEM # EM\n",
    "    ])\n",
    "    \n",
    "states_config = [\n",
    "    {'label': 'T', 'initial_value': 4E8},\n",
    "    {'label': 'I1', 'initial_value': 75.0},\n",
    "    {'label': 'I2', 'initial_value': 0.0},\n",
    "    {'label': 'V', 'initial_value': 0.0},\n",
    "    {'label': 'E', 'initial_value': 0.0},\n",
    "    {'label': 'EM', 'initial_value': 0.0}\n",
    "]\n",
    "\n",
    "parameters_config = Parameters(\n",
    "    beta=Parameter(name='beta', val=8.14E-6, l_lim=3E-6, u_lim=4.8E-4, mode='file'),\n",
    "    k=Parameter(name='k', val=4.0, l_lim=3.5, u_lim=6.0, mode='fixed'),\n",
    "    p=Parameter(name='p', val=0.78, l_lim=0.1, u_lim=3, mode='file'),\n",
    "    c=Parameter(name='c', val=4.5, l_lim=1, u_lim=16, mode='file'),\n",
    "    delta=Parameter(name='delta', val=11.71, l_lim=2.4, u_lim=22, mode='file'),\n",
    "    delta_E=Parameter(name='delta_E', val=40, l_lim=0.1, u_lim=3.0, mode='fixed'),\n",
    "    Kd=Parameter(name='Kd', val=434, l_lim=1E2, u_lim=1E5, mode='fixed'),\n",
    "    xi=Parameter(name='xi', val=5E4, l_lim=1E2, u_lim=1E5, mode='fit'),\n",
    "    K_Ef=Parameter(name='K_Ef', val=1E5, l_lim=1E3, u_lim=1E6, mode='fixed'),\n",
    "    eta=Parameter(name='eta', val=2E-7, l_lim=2E-7, u_lim=3E-7, mode='fit'),\n",
    "    tau_E=Parameter(name='tau_E', val=2.0, l_lim=3.0, u_lim=4.0, mode='fixed'),\n",
    "    d_E=Parameter(name='d_E', val=1.0, l_lim=0.5, u_lim=2.0, mode='fixed'),\n",
    "    zet=Parameter(name='zet', val=0.22, l_lim=0.01, u_lim=0.5, mode='fixed'),\n",
    "    tau_EM=Parameter(name='tau_EM', val=3.5, l_lim=3.0, u_lim=4.0, mode='fixed')\n",
    ")\n",
    "\n",
    "parameters_file_path = \"../NLME/Baccam_individual_parameters.xlsx\"\n",
    "parameters_list = parameters_config.load_parameters_from_file(parameters_file_path, usecols=\"A:F\") \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "   results = execute(MonoModel, parameters_list, states_config, t_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'V' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 148\u001b[0m\n\u001b[0;32m    142\u001b[0m subplot_configuration \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    143\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVirus\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_key\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msol_key\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_scale\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mylims\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1E9\u001b[39m]},\n\u001b[0;32m    144\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCD8 T Effectors\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_key\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msol_key\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_scale\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5.6E7\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mylims\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1E5\u001b[39m, \u001b[38;5;241m1E7\u001b[39m]}\n\u001b[0;32m    145\u001b[0m ]\n\u001b[0;32m    147\u001b[0m volunteer_offsets \u001b[38;5;241m=\u001b[39m preprocess_data(best_Rahil, subplot_configuration, best_ids)\n\u001b[1;32m--> 148\u001b[0m interpolated_results \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_ode_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubplot_configuration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_span\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvolunteer_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m sse_results \u001b[38;5;241m=\u001b[39m compute_all_sses(best_Rahil, subplot_configuration, interpolated_results)\n\u001b[0;32m    150\u001b[0m total_sse_by_id, total_sse_by_state, grand_total_sse, total_data_points, formatted_sse_results \u001b[38;5;241m=\u001b[39m format_sse_results(sse_results)\n",
      "Cell \u001b[1;32mIn[20], line 70\u001b[0m, in \u001b[0;36mpreprocess_ode_results\u001b[1;34m(ode_results, subplot_config, t_span, volunteer_offsets)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m subplot_config:\n\u001b[0;32m     69\u001b[0m     state_label \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msol_key\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 70\u001b[0m     interp_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minterp(t_span, ode_result\u001b[38;5;241m.\u001b[39mt, ode_result\u001b[38;5;241m.\u001b[39my[\u001b[43mstates_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_label\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[0;32m     71\u001b[0m     interp_data[state_label] \u001b[38;5;241m=\u001b[39m interp_values \u001b[38;5;241m+\u001b[39m volunteer_offsets[volunteer_id]\u001b[38;5;241m.\u001b[39mget(state_label, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     72\u001b[0m interpolated_results[volunteer_id] \u001b[38;5;241m=\u001b[39m interp_data\n",
      "\u001b[1;31mValueError\u001b[0m: 'V' is not in list"
     ]
    }
   ],
   "source": [
    "def compute_means(data, config, scale_factor=1.0):\n",
    "    if scale_factor is None:\n",
    "        scale_factor = 1.0\n",
    "    data_copy = data.copy()\n",
    "    data_copy.loc[:, 'scaled_data'] = data_copy[config['data_key']] * scale_factor\n",
    "    data_copy.loc[:, 'log_data'] = np.log10(data_copy['scaled_data'].replace(0, np.nan))  # Avoid log10(0)\n",
    "    grouped_volunteer_data = data_copy.groupby('DAY')['log_data'].agg(['mean', 'std']).reset_index()\n",
    "    grouped_volunteer_data['mean_exp'] = 10 ** grouped_volunteer_data['mean']\n",
    "    grouped_volunteer_data['std_exp_lower'] = 10 ** (grouped_volunteer_data['mean'] - grouped_volunteer_data['std'])\n",
    "    grouped_volunteer_data['std_exp_upper'] = 10 ** (grouped_volunteer_data['mean'] + grouped_volunteer_data['std'])\n",
    "    return grouped_volunteer_data\n",
    "\n",
    "def compute_ode_means(ode_results, config, states_config, t_span, volunteer_offsets, ids):\n",
    "    sol_list = ode_results[\"sol_list\"]\n",
    "    states = States(states_config)\n",
    "    sol_data = []\n",
    "\n",
    "    interpolated_times = np.unique(np.concatenate((np.linspace(t_span[0], t_span[-1], num=200), t_span[1:-2])))\n",
    "\n",
    "    for index, (sol_t, sol_y) in enumerate(sol_list):\n",
    "        if index<len(ids):\n",
    "            key_index = states.state_labels.index(config['sol_key'])\n",
    "            sol_values = sol_y[key_index, :]\n",
    "\n",
    "            volunteer_id = ids[index]\n",
    "            if config['sol_key'] != 'V':\n",
    "                offset = volunteer_offsets[volunteer_id][config['sol_key']]\n",
    "                sol_values += offset\n",
    "\n",
    "            interpolated_values = np.interp(interpolated_times, sol_t, sol_values)\n",
    "            interpolated_log_values = np.log10(np.maximum(interpolated_values, 0.01))\n",
    "            sol_data.append(interpolated_log_values)  # Store log-transformed values\n",
    "\n",
    "    sol_data_array = np.stack(sol_data)\n",
    "    mean_sol_log = np.mean(sol_data_array, axis=0)\n",
    "    std_sol_log = np.std(sol_data_array, axis=0)\n",
    "\n",
    "    mean_sol_nat = 10**mean_sol_log\n",
    "    std_plus_std_nat = 10**(mean_sol_log + std_sol_log)\n",
    "    std_minus_std_nat = 10**(mean_sol_log - std_sol_log)\n",
    "\n",
    "    return interpolated_times, mean_sol_nat, std_minus_std_nat, std_plus_std_nat\n",
    "\n",
    "def extract_offsets(data, subplot_config, ids):\n",
    "    offsets = {volunteer: {} for volunteer in ids}\n",
    "    for config in subplot_config:\n",
    "        if config['sol_key'] != 'V':  # Skip virus\n",
    "            for volunteer in ids:\n",
    "                volunteer_data = data[data['VOLUNTEER'] == volunteer]\n",
    "                scale_factor = config.get('data_scale', 1.0)\n",
    "                y_limits = config.get('ylims', [0, np.inf])\n",
    "                scaled_data = scale_factor * volunteer_data[config['data_key']]\n",
    "                \n",
    "                valid_values = scaled_data[(scaled_data > y_limits[0]) & (scaled_data < y_limits[1])]\n",
    "                if not valid_values.empty:\n",
    "                    first_valid_value = valid_values.iloc[0]\n",
    "                    offsets[volunteer][config['sol_key']] = first_valid_value\n",
    "    return offsets\n",
    "\n",
    "def preprocess_data(data, subplot_config, ids):\n",
    "    volunteer_offsets = extract_offsets(data, subplot_config, ids)\n",
    "    return volunteer_offsets\n",
    "\n",
    "def preprocess_ode_results(ode_results, subplot_config, t_span, volunteer_offsets):\n",
    "    interpolated_results = {}\n",
    "    for idx, (volunteer_id, ode_result) in enumerate(zip(best_ids, ode_results['sol_list'])):\n",
    "        interp_data = {}\n",
    "        for config in subplot_config:\n",
    "            state_label = config['sol_key']\n",
    "            interp_values = np.interp(t_span, ode_result.t, ode_result.y[states_config.index(state_label)])\n",
    "            interp_data[state_label] = interp_values + volunteer_offsets[volunteer_id].get(state_label, 0)\n",
    "        interpolated_results[volunteer_id] = interp_data\n",
    "    return interpolated_results\n",
    "\n",
    "def compute_sse(volunteer_data, ode_times, ode_values, y_limits):\n",
    "    sse = 0\n",
    "    count = 0  # To keep track of valid points for SSE calculation\n",
    "    \n",
    "    for time_point, data_point in zip(volunteer_data['DAY'], volunteer_data['scaled_data']):\n",
    "        if time_point == 0:\n",
    "            continue  # Skip initial condition points\n",
    "        if y_limits[0] <= data_point <= y_limits[1]:\n",
    "            ode_value = np.interp(time_point, ode_times, ode_values)\n",
    "            \n",
    "            if data_point <= 0:\n",
    "                data_point = np.nan\n",
    "            \n",
    "            if ode_value <= 0:\n",
    "                ode_value = np.nan\n",
    "            \n",
    "            if not np.isnan(data_point) and not np.isnan(ode_value):\n",
    "                sse += (np.log10(data_point) - np.log10(ode_value)) ** 2\n",
    "                count += 1\n",
    "    \n",
    "    if count == 0:\n",
    "        return np.inf, 0  # No valid data/ODE\n",
    "    else:\n",
    "        return sse, count\n",
    "\n",
    "def compute_all_sses(data, subplot_config, interpolated_results):\n",
    "    ids = data['VOLUNTEER'].unique()\n",
    "    sse_results = {volunteer: {} for volunteer in ids}\n",
    "    \n",
    "    for volunteer in ids:\n",
    "        for config in subplot_config:\n",
    "            scale_factor = config.get('data_scale', 1.0) if config['data_key'] else 1.0\n",
    "            if scale_factor is None:\n",
    "                scale_factor = 1.0\n",
    "            if config['data_key']:\n",
    "                volunteer_data = data[data['VOLUNTEER'] == volunteer].copy()\n",
    "                volunteer_data['scaled_data'] = volunteer_data[config['data_key']] * scale_factor  # Apply scaling\n",
    "                \n",
    "                if config['sol_key']:\n",
    "                    interpolated_values_for_sse = interpolated_results[volunteer][config['sol_key']]  # mean_sol_nat\n",
    "                    y_limits = [1, np.inf] if 'ylims' not in config else config['ylims']\n",
    "                    sse, count = compute_sse(volunteer_data, t_span, interpolated_values_for_sse, y_limits)\n",
    "                    sse_results[volunteer][config['title']] = {'sse': round(sse, 2), 'count': count}\n",
    "    \n",
    "    return sse_results\n",
    "\n",
    "def format_sse_results(sse_results):\n",
    "    total_sse_by_id = {}\n",
    "    total_sse_by_state = {}\n",
    "    grand_total_sse = 0\n",
    "    total_data_points = 0\n",
    "    \n",
    "    for volunteer, states in sse_results.items():\n",
    "        total_sse_by_id[volunteer] = round(sum(state['sse'] for state in states.values()), 2)\n",
    "        for state, result in states.items():\n",
    "            if state not in total_sse_by_state:\n",
    "                total_sse_by_state[state] = {'sse': 0, 'count': 0}\n",
    "            total_sse_by_state[state]['sse'] += result['sse']\n",
    "            total_sse_by_state[state]['count'] += result['count']\n",
    "            grand_total_sse += result['sse']\n",
    "            total_data_points += result['count']\n",
    "    \n",
    "    total_sse_by_state = {k: {'sse': round(v['sse'], 2), 'count': v['count']} for k, v in total_sse_by_state.items()}\n",
    "    grand_total_sse = round(grand_total_sse, 2)\n",
    "    \n",
    "    return total_sse_by_id, total_sse_by_state, grand_total_sse, total_data_points, sse_results\n",
    "\n",
    "subplot_configuration = [\n",
    "    {'title': 'Virus', 'data_key': 'V', 'sol_key': 'V', 'data_scale': None, 'ylims': [1,1E9]},\n",
    "    {'title': 'CD8 T Effectors', 'data_key': 'E', 'sol_key': 'E', 'data_scale': 5.6E7, 'ylims': [1E5, 1E7]}\n",
    "]\n",
    "\n",
    "volunteer_offsets = preprocess_data(best_Rahil, subplot_configuration, best_ids)\n",
    "interpolated_results = preprocess_ode_results(results, subplot_configuration, t_span, volunteer_offsets)\n",
    "sse_results = compute_all_sses(best_Rahil, subplot_configuration, interpolated_results)\n",
    "total_sse_by_id, total_sse_by_state, grand_total_sse, total_data_points, formatted_sse_results = format_sse_results(sse_results)\n",
    "\n",
    "# Display Results\n",
    "sse_by_id_state_df = pd.DataFrame.from_dict({(i,j): formatted_sse_results[i][j] \n",
    "                            for i in formatted_sse_results.keys() \n",
    "                            for j in formatted_sse_results[i].keys()},\n",
    "                        orient='index')\n",
    "\n",
    "sse_by_id_df = pd.DataFrame.from_dict(total_sse_by_id, orient='index', columns=['Total SSE'])\n",
    "sse_by_state_df = pd.DataFrame.from_dict(total_sse_by_state, orient='index').rename(columns={'sse': 'Total SSE', 'count': 'Data Points'})\n",
    "\n",
    "print(\"\\nSSE Results by State:\")\n",
    "print(sse_by_state_df.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(data, ode_results, subplot_config, color_dict, data_means, ode_means, interpolated_times, means=True, individual_plots=False):   \n",
    "    ids = data['VOLUNTEER'].unique()\n",
    "    t_span = [0, 8]\n",
    "    x_ticks = np.arange(t_span[0], t_span[1], 1)\n",
    "    n_subplots = len(subplot_config)\n",
    "    n_rows, n_cols = int(np.ceil(np.sqrt(n_subplots))), int(np.ceil(n_subplots / np.sqrt(n_subplots)))\n",
    "    base_font_size = 12\n",
    "\n",
    "    if individual_plots:\n",
    "        for volunteer in ids:\n",
    "            fig, axs = plt.subplots(n_rows, n_cols, figsize=(9, 9), squeeze=False)\n",
    "            axs = axs.flatten()\n",
    "            \n",
    "            for i, config in enumerate(subplot_config):\n",
    "                ax = axs[i]\n",
    "                ax.set_title(config['title'], fontsize=base_font_size + 2)\n",
    "                scale_factor = config.get('data_scale', 1.0) if config['data_key'] else 1.0\n",
    "                if scale_factor is None: scale_factor = 1.0\n",
    "\n",
    "                if means and config['data_key']:\n",
    "                    mean_data = data_means[config['title']]\n",
    "                    ax.errorbar(mean_data['DAY'], mean_data['mean_exp'], \n",
    "                                yerr=[mean_data['mean_exp'] - mean_data['std_exp_lower'],\n",
    "                                      mean_data['std_exp_upper'] - mean_data['mean_exp']], \n",
    "                                fmt='o', color='black', capsize=3, label='Mean ± STD', alpha=0.4)\n",
    "\n",
    "                if config['data_key']:\n",
    "                    volunteer_data = data[data['VOLUNTEER'] == volunteer].copy()\n",
    "                    volunteer_data['scaled_data'] = volunteer_data[config['data_key']] * scale_factor  # Apply scaling\n",
    "                    ax.scatter(volunteer_data['DAY'], volunteer_data['scaled_data'], color=color_dict[volunteer], alpha=1.0, zorder=5)\n",
    "\n",
    "                if means and config['sol_key']:\n",
    "                    interpolated_times, mean_sol_nat, std_minus_std_nat, std_plus_std_nat = ode_means[config['title']]\n",
    "                    ax.plot(interpolated_times, mean_sol_nat, 'k-', lw=1, alpha=0.5, zorder=5)  # Mean solution\n",
    "                    ax.fill_between(interpolated_times, std_minus_std_nat, std_plus_std_nat, color='gray', alpha=0.2)  # Standard deviation\n",
    "\n",
    "                if config['sol_key']:\n",
    "                    sol_list = ode_results[\"sol_list\"]\n",
    "                    states = States(states_config)\n",
    "                    \n",
    "                    for index, (sol_t, sol_y) in enumerate(sol_list):\n",
    "                        if index<len(ids):\n",
    "                            if ids[index] == volunteer:\n",
    "                                key_index = states.state_labels.index(config['sol_key'])\n",
    "                                sol_values = sol_y[key_index, :]\n",
    "                                color = color_dict[volunteer]\n",
    "                                interpolated_values = np.interp(interpolated_times, sol_t, sol_values)\n",
    "                                \n",
    "                                ax.plot(interpolated_times, interpolated_values, color=color, alpha=0.5)\n",
    "\n",
    "                                if 'interpolated_values' not in globals():\n",
    "                                    globals()['interpolated_values'] = {}\n",
    "                                if volunteer not in globals()['interpolated_values']:\n",
    "                                    globals()['interpolated_values'][volunteer] = {}\n",
    "                                globals()['interpolated_values'][volunteer][config['sol_key']] = interpolated_values\n",
    "\n",
    "                y_limits = ax.get_ylim()\n",
    "                if config['data_key']:\n",
    "                    interpolated_values_for_sse = globals()['interpolated_values'][volunteer].get(config['sol_key'], np.zeros_like(interpolated_times))\n",
    "                    sse, count = compute_sse(volunteer_data, interpolated_times, interpolated_values_for_sse, y_limits)\n",
    "                    if count > 0:\n",
    "                        legend_label = f\"{volunteer}: SSE={sse:.2f} (n={count})\"\n",
    "                        ax.scatter(volunteer_data['DAY'], volunteer_data['scaled_data'], color=color_dict[volunteer], label=legend_label, alpha=1.0, zorder=5)\n",
    "\n",
    "                if config['ylims']:\n",
    "                    ax.set_ylim(config['ylims'])\n",
    "                ax.set_yscale('log')\n",
    "                y_low, y_high = ax.get_ylim()\n",
    "                if y_low < 1:\n",
    "                    ax.set_ylim(bottom=1)\n",
    "                ax.set_xticks(x_ticks)\n",
    "                ax.set_xlim(t_span)\n",
    "                \n",
    "                ax.margins(x=0.05)\n",
    "                ax.set_xlabel('Days Post Infection')\n",
    "                ax.set_ylabel('Level')\n",
    "                ax.legend()\n",
    "\n",
    "            for j in range(i + 1, len(axs)):\n",
    "                axs[j].set_visible(False)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        fig, axs = plt.subplots(n_rows, n_cols, figsize=(9, 9), squeeze=False)\n",
    "        axs = axs.flatten()\n",
    "\n",
    "        for i, config in enumerate(subplot_config):\n",
    "            ax = axs[i]\n",
    "            ax.set_title(config['title'], fontsize=base_font_size + 2)\n",
    "\n",
    "            if config['data_key'] and config['data_key'] in data.columns:\n",
    "                scale_factor = config.get('data_scale', 1.0) if config['data_key'] else 1.0\n",
    "                if scale_factor is None: scale_factor = 1.0\n",
    "                for volunteer, color in color_dict.items():\n",
    "                    volunteer_data = data[data['VOLUNTEER'] == volunteer].copy()\n",
    "                    volunteer_data['scaled_data'] = volunteer_data[config['data_key']] * scale_factor  # Apply scaling\n",
    "                    ax.scatter(volunteer_data['DAY'], volunteer_data['scaled_data'], color=color, alpha=1.0, zorder=5)\n",
    "\n",
    "                if means:\n",
    "                    mean_data = data_means[config['title']]\n",
    "                    ax.errorbar(mean_data['DAY'], mean_data['mean_exp'], \n",
    "                                yerr=[mean_data['mean_exp'] - mean_data['std_exp_lower'],\n",
    "                                      mean_data['std_exp_upper'] - mean_data['mean_exp']], \n",
    "                                fmt='o', color='black', capsize=5, label='Mean ± STD', alpha=0.4)\n",
    "\n",
    "            if config['sol_key'] and ode_results:\n",
    "                sol_list = ode_results[\"sol_list\"]\n",
    "                states = States(states_config)\n",
    "                \n",
    "                for index, (sol_t, sol_y) in enumerate(sol_list):\n",
    "                    key_index = states.state_labels.index(config['sol_key'])\n",
    "                    sol_values = sol_y[key_index, :]  \n",
    "                    ode_id = ids[index]\n",
    "                    color = color_dict[ode_id]\n",
    "                    \n",
    "                    interpolated_values = np.interp(interpolated_times, sol_t, sol_values)\n",
    "                    \n",
    "                    ax.plot(interpolated_times, interpolated_values, color=color, alpha=0.5)\n",
    "\n",
    "                    if 'interpolated_values' not in globals():\n",
    "                        globals()['interpolated_values'] = {}\n",
    "                    if ode_id not in globals()['interpolated_values']:\n",
    "                        globals()['interpolated_values'][ode_id] = {}\n",
    "                    globals()['interpolated_values'][ode_id][config['sol_key']] = interpolated_values\n",
    "\n",
    "                if means:\n",
    "                    interpolated_times, mean_sol_nat, std_minus_std_nat, std_plus_std_nat = ode_means[config['title']]\n",
    "                    ax.plot(interpolated_times, mean_sol_nat, 'k-', lw=1, alpha=0.5)  # Mean solution\n",
    "                    ax.fill_between(interpolated_times, std_minus_std_nat, std_plus_std_nat, color='gray', alpha=0.25)  # Standard deviation\n",
    "\n",
    "            y_limits = ax.get_ylim()\n",
    "            if config['data_key']:\n",
    "                for volunteer, color in color_dict.items():\n",
    "                    volunteer_data = data[data['VOLUNTEER'] == volunteer].copy()\n",
    "                    volunteer_data['scaled_data'] = volunteer_data[config['data_key']] * scale_factor  # Apply scaling\n",
    "                    interpolated_values_for_sse = globals()['interpolated_values'][volunteer].get(config['sol_key'], np.zeros_like(interpolated_times))\n",
    "                    sse, count = compute_sse(volunteer_data, interpolated_times, interpolated_values_for_sse, y_limits)\n",
    "                    if count > 0:\n",
    "                        legend_label = f\"{volunteer}: SSE={sse:.2f} (n={count})\"\n",
    "                        ax.scatter(volunteer_data['DAY'], volunteer_data['scaled_data'], color=color, label=legend_label, alpha=1.0, zorder=5)\n",
    "\n",
    "            if config['ylims']:\n",
    "                ax.set_ylim(config['ylims']) \n",
    "            ax.set_yscale('log')\n",
    "            y_low, y_high = ax.get_ylim()\n",
    "            if y_low < 1:\n",
    "                ax.set_ylim(bottom=1)\n",
    "            ax.set_xticks(x_ticks)\n",
    "            ax.set_xlim(t_span)\n",
    "            \n",
    "            ax.margins(x=0.05)\n",
    "            ax.set_xlabel('Days Post Infection')\n",
    "            ax.set_ylabel('Level')\n",
    "\n",
    "        for j in range(i + 1, len(axs)):\n",
    "            axs[j].set_visible(False)\n",
    "        plt.tight_layout()\n",
    "        plt.show()  \n",
    "        \n",
    "plot(best_Rahil, ode_results, subplot_configuration, best_color_dict, data_means, ode_means, interpolated_times, means=True, individual_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monolith\n",
    "def MonoModel(t: float, y: np.ndarray, p: Parameters, s: States, time_manager: TimeManager, timeout: int = None) -> np.ndarray:\n",
    "    if timeout is not None and time_manager.check_timeout(timeout): \n",
    "        raise TimeoutError('ODE Solver timeout.')\n",
    "\n",
    "    y[:] = y.clip(min=0.0)\n",
    "    for label, value in zip(s.state_labels, y):\n",
    "        s.states[label].update_value(t, value)\n",
    "\n",
    "    s.calculate_delayed_states(t, [('tau_E', 'E', 'I2'), \n",
    "                                ('tau_EM', 'EM', 'E'),\n",
    "                                #('tau_A', 'A', 'I2'),\n",
    "                                #('tau_B', 'B', 'I1'),\n",
    "                                #('tau_CA', 'CA', 'I2'),\n",
    "                                #('tau_CB', 'CB', 'I1')\n",
    "                                ], p)\n",
    "    I2_tauE = s.get_delayed_state('I2_E')\n",
    "    #I2_tauA = s.get_delayed_state('I2_A')\n",
    "    #I1_tauB = s.get_delayed_state('I1_B')\n",
    "    #I2_tauCA = s.get_delayed_state('I2_CA')\n",
    "    #I1_tauCB = s.get_delayed_state('I1_CB')\n",
    "    E_tauEM = s.get_delayed_state('E_EM') \n",
    "\n",
    "    return np.array([\n",
    "        -p.beta.val * s.T * s.V,  # T\n",
    "        p.beta.val * s.T * s.V - p.k.val * s.I1,  # I1\n",
    "        p.k.val * s.I1 - p.delta.val * s.I2, #- p.delta_E.val * s.E * s.I2 / (p.Kd.val + s.I2),  # I2\n",
    "        p.p.val * s.I2 - p.c.val * s.V,  # V\n",
    "        p.xi.val * s.I2 / (p.K_Ef.val + s.E) + p.eta.val * s.E * I2_tauE - p.d_E.val * s.E,  # E\n",
    "        p.zet.val * E_tauEM, # EM\n",
    "        #p.alpha.val * I2_tauA + p.gamma.val * s.CA - p.dA.val * s.A, # A\n",
    "        #p.iota.val * I1_tauB + p.kappa.val * s.CB - p.dB.val * s.B, # B\n",
    "        #p.theta.val * I2_tauCA - p.dCA.val * s.CA, # CA\n",
    "        #p.lamda.val * I1_tauCB - p.dCB.val * s.CB # CB\n",
    "        \n",
    "    ])\n",
    "    \n",
    "parameters = Parameters(\n",
    "    beta=Parameter(name='beta', val=8.14E-6, l_lim=3E-6, u_lim=4.8E-4, mode='file'),\n",
    "    k=Parameter(name='k', val=4.0, l_lim=3.5, u_lim=6.0, mode='file'),\n",
    "    p=Parameter(name='p', val=0.78, l_lim=0.1, u_lim=3, mode='file'),\n",
    "    c=Parameter(name='c', val=4.5, l_lim=1, u_lim=16, mode='file'),\n",
    "    delta=Parameter(name='delta', val=11.71, l_lim=2.4, u_lim=22, mode='file'),\n",
    "    delta_E=Parameter(name='delta_E', val=40, l_lim=0.1, u_lim=3.0, sample=False),\n",
    "    Kd=Parameter(name='Kd', val=434, l_lim=1E2, u_lim=1E5, sample=False),\n",
    "    xi=Parameter(name='xi', val=5E4, l_lim=1E2, u_lim=1E5, sample=False),\n",
    "    K_Ef=Parameter(name='K_Ef', val=1E5, l_lim=1E3, u_lim=1E6, sample=False),\n",
    "    eta=Parameter(name='eta', val=2E-7, l_lim=2E-7, u_lim=3E-7, sample=False),\n",
    "    tau_E=Parameter(name='tau_E', val=2.0, l_lim=3.0, u_lim=4.0, sample=False),\n",
    "    d_E=Parameter(name='d_E', val=1.0, l_lim=0.5, u_lim=2.0, sample=False),\n",
    "    zet=Parameter(name='zet', val=0.22, l_lim=0.01, u_lim=0.5, sample=False),\n",
    "    tau_EM=Parameter(name='tau_EM', val=3.5, l_lim=3.0, u_lim=4.0, sample=False),\n",
    "    #alpha=Parameter(name='alpha', val=2.75E-3, l_lim=1E-4, u_lim=1E-2, sample=False),\n",
    "    #gamma=Parameter(name='gamma', val=7.5, l_lim=1E-2, u_lim=1E1, sample=False),\n",
    "    #tau_A=Parameter(name='tau_A', val=0.75, l_lim=3.0, u_lim=4.0, sample=False),\n",
    "    #dA=Parameter(name='dA', val=75.0, l_lim=40.0, u_lim=100.0, sample=False),\n",
    "    #iota=Parameter(name='iota', val=2.5E-5, l_lim=0.01, u_lim=0.5, sample=False),\n",
    "    #kappa=Parameter(name='kappa', val=0.1, l_lim=0.01, u_lim=0.5, sample=False),\n",
    "    #tau_B=Parameter(name='tau_B', val=0.85, l_lim=3.0, u_lim=5.0, sample=False),\n",
    "    #dB=Parameter(name='dB', val=1.0, l_lim=0.5, u_lim=5.0, sample=False),\n",
    "    #theta=Parameter(name='theta', val=1E-3, l_lim=1E-4, u_lim=1E-2, sample=False),\n",
    "    #dCA=Parameter(name='dCA', val=10.0, l_lim=0.5, u_lim=2.0, sample=False),\n",
    "    #lamda=Parameter(name='lamda', val=1E-3, l_lim=1E-4, u_lim=1E-2, sample=False),\n",
    "    #tau_CA=Parameter(name='tau_CA', val=6.0, l_lim=3.0, u_lim=5.0, sample=False),\n",
    "    #dCB=Parameter(name='dCB', val=9.4, l_lim=0.5, u_lim=2.0, sample=False),\n",
    "    #tau_CB=Parameter(name='tau_CB', val=2.5, l_lim=3.0, u_lim=5.0, sample=False)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
